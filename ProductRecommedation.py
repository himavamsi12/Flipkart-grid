# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cTtKsyLKak6AEfKH0mcE3mnUyqyGuyAp
"""

import numpy as np
from sklearn.metrics import mean_squared_error
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Embedding, Flatten, Dense, Input, Concatenate

# Simulated data parameters
num_users = 1000
num_products = 500
num_interactions = 5000

# Generate user profiles and interactions
users = pd.DataFrame({'user_id': np.arange(num_users)})
products = pd.DataFrame({'product_id': np.arange(num_products)})
user_profiles = pd.DataFrame({
    'user_id': np.random.choice(users['user_id'], num_interactions),
    'product_id': np.random.choice(products['product_id'], num_interactions),
    'rating': np.random.randint(1, 6, num_interactions)
})

# Create interaction matrix
interaction_matrix = user_profiles.pivot_table(index='user_id', columns='product_id', values='rating').fillna(0)

# Calculate user-user similarity
user_similarity = cosine_similarity(interaction_matrix)

def collaborative_filtering_model(num_users, num_products, embedding_size=50):
    user_input = Input(shape=(1,))
    product_input = Input(shape=(1,))

    user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size)(user_input)
    product_embedding = Embedding(input_dim=num_products, output_dim=embedding_size)(product_input)

    user_flat = Flatten()(user_embedding)
    product_flat = Flatten()(product_embedding)

    dot_product = tf.keras.layers.dot([user_flat, product_flat], axes=1, normalize=False)

    model = Model(inputs=[user_input, product_input], outputs=dot_product)
    model.compile(optimizer='adam', loss='mean_squared_error')

    return model

# Prepare data for training
X = user_profiles[['user_id', 'product_id']]  # Use both user_id and product_id
y = user_profiles['rating']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Prepare data for training
X_train_user = X_train['user_id'].values
X_train_product = X_train['product_id'].values

X_test_user = X_test['user_id'].values
X_test_product = X_test['product_id'].values

# Train the model
# Temporarily enable eager execution
tf.config.experimental_run_functions_eagerly(True)

# Train the model
cf_model.fit([X_train_user, X_train_product], y_train, epochs=10, batch_size=32, validation_data=([X_test_user, X_test_product], y_test))

# Disable eager execution
tf.config.experimental_run_functions_eagerly(False)

def neural_collaborative_filtering_model(num_users, num_products, embedding_size=50, hidden_units=[64, 32, 16]):
    user_input = Input(shape=(1,))
    product_input = Input(shape=(1,))

    user_embedding = Embedding(input_dim=num_users, output_dim=embedding_size)(user_input)
    product_embedding = Embedding(input_dim=num_products, output_dim=embedding_size)(product_input)

    user_flat = Flatten()(user_embedding)
    product_flat = Flatten()(product_embedding)

    concat_layer = Concatenate()([user_flat, product_flat])

    for units in hidden_units:
        concat_layer = Dense(units, activation='relu')(concat_layer)

    output = Dense(1, activation='relu')(concat_layer)

    model = Model(inputs=[user_input, product_input], outputs=output)
    model.compile(optimizer='adam', loss='mean_squared_error')

    return model

cf_model = collaborative_filtering_model(num_users, num_products)
cf_model.compile(optimizer='adam', loss='mean_squared_error')
cf_model.fit([X_train_user, X_train_product], y_train, epochs=10, batch_size=32, validation_data=([X_test_user, X_test_product], y_test))

# Train the neural collaborative filtering model
ncf_model = neural_collaborative_filtering_model(num_users, num_products)
ncf_model.compile(optimizer='adam', loss='mean_squared_error')
ncf_model.fit([X_train_user, X_train_product], y_train, epochs=10, batch_size=32, validation_data=([X_test_user, X_test_product], y_test))

# Evaluate the models
cf_loss = cf_model.evaluate([X_test_user, X_test_product], y_test, verbose=0)
ncf_loss = ncf_model.evaluate([X_test_user, X_test_product], y_test, verbose=0)

print("Collaborative Filtering Model Loss:", cf_loss)
print("Neural Collaborative Filtering Model Loss:", ncf_loss)

cf_predictions = cf_model.predict([X_test_user, X_test_product])
ncf_predictions = ncf_model.predict([X_test_user, X_test_product])

cf_rmse = np.sqrt(mean_squared_error(y_test, cf_predictions))
ncf_rmse = np.sqrt(mean_squared_error(y_test, ncf_predictions))

print("Collaborative Filtering Model RMSE:", cf_rmse)
print("Neural Collaborative Filtering Model RMSE:", ncf_rmse)

# Sample user_id for recommendation
sample_user_id = 10

# Get the products the sample user has interacted with
sample_user_interactions = user_profiles[user_profiles['user_id'] == sample_user_id]['product_id'].unique()

# Create a list of all product_ids
all_product_ids = np.arange(num_products)

# Filter out products the sample user has already interacted with
products_to_recommend = np.setdiff1d(all_product_ids, sample_user_interactions)

# Create a DataFrame with user_id and recommended product_id pairs
recommendation_data = pd.DataFrame({
    'user_id': np.full(len(products_to_recommend), sample_user_id),
    'product_id': products_to_recommend
})

# Predict ratings for recommended products using the collaborative filtering model
cf_predicted_ratings = cf_model.predict([recommendation_data['user_id'], recommendation_data['product_id']])

# Predict ratings for recommended products using the neural collaborative filtering model
ncf_predicted_ratings = ncf_model.predict([recommendation_data['user_id'], recommendation_data['product_id']])

# Combine user_id, product_id, and predicted ratings
recommendations = pd.DataFrame({
    'user_id': recommendation_data['user_id'],
    'product_id': recommendation_data['product_id'],
    'cf_predicted_rating': cf_predicted_ratings.flatten(),
    'ncf_predicted_rating': ncf_predicted_ratings.flatten()
})

# Get top N recommendations based on predicted ratings
top_n = 10
top_cf_recommendations = recommendations.nlargest(top_n, 'cf_predicted_rating')
top_ncf_recommendations = recommendations.nlargest(top_n, 'ncf_predicted_rating')

print("Top Collaborative Filtering Recommendations:")
print(top_cf_recommendations)

print("\nTop Neural Collaborative Filtering Recommendations:")
print(top_ncf_recommendations)
